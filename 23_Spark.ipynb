{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python for Data Science & Analysis \n",
    "## Notes 3.1, Spark & SQL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "* import the pyspark v.2 library\n",
    "* connected to a spark instance\n",
    "* read & parse a csv file\n",
    "* write a select query:\n",
    "    * over all columns\n",
    "    * over a subset of columns\n",
    "    * over two tables\n",
    "    * filters by rows \n",
    "    * subsets rows and aggregates \n",
    "* EXTRA: write and use a *python* user-defined function in SparkSQL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is distributed computing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With spark, datasets are distributed over a large number of machines in a file-system-like way.  This set of machines (\"cluster\") behaves like a single file system. \n",
    "\n",
    "Contrast this with a database where a schema is typically imposed on all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why distribute computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When the data does not fit on a single machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Two situations:\n",
    "1. doesn't fit into RAM and needs to\n",
    "    * eg., image processing\n",
    "2. doesn't fit into solid disk\n",
    "    * eg., massive datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* RAM\n",
    "    * apple.com Mac Pro, 2TB\n",
    "    * amazon.com buy 256gb *4 = 1TB  \n",
    "* Hard/Solid disks\n",
    "    * Seagate 16TB IronWolf -- a single 16TB disk\n",
    "    * easily have many disks, so, 1PB\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "NB. These numbers are **different** today, than **ten years ago**. eg., 100GB+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should I not distribute computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a query **can** be performed on a single machine, it is almost always faster to do so.\n",
    "\n",
    "When dealing with distributed querying, you require machines to synchronize over a network -- and to cooridinate, etc., and this typically imposes high initial costs / query. \n",
    "\n",
    "NB. *always* prototype systems first, and experiment with query performance before selecting complicated solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a system  \n",
    "    * a means of distributing data across a large number of machiens\n",
    "    * a means of querying across these\n",
    "        * assuming no underlying data structure\n",
    "* a library, `pyspark`\n",
    "    * for using this system\n",
    "* two programs which implement the system\n",
    "    * executor program\n",
    "        * runs on all data machines\n",
    "        * program which runs queries\n",
    "    * driver\n",
    "        * runs on developer machine (, laptop, etc.)\n",
    "        * sends queries out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I try spark without an installation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Register for Community Edition\n",
    "* https://databricks.com/try-databricks\n",
    "    * register for **COMMUNITY** edition!\n",
    "    * ie., press the \"GET STARTED\" button \n",
    "    * register \n",
    "        * (requries valid email, but you can use a fake email system)\n",
    "* when registered you will get a email\n",
    "    * click on the link\n",
    "    * you will be asked to provide a password\n",
    "    * and then direct to:\n",
    "    * https://community.cloud.databricks.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Go to community.cloud.databricks.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/1.png  width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Select `Clusters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select `Clusters` from the menu on the left-hand side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/2.png  width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Create a Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click `+ Create Cluster`, then fill in any name, eg., `dummy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/3.png  width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press **\"CREATE CLUSTER\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Add Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select `Data` from the left-hand-side, and then press the `Create Table` button (top right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/4.png  width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Upload a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on `browse` **link** to find the `csv` file..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/5a.png  width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Browse your system for the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the `titanic.csv`, file in the courseware `datasets` folder..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/5b.png  width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Create a Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When uploading is complete, press `Create Table in Notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/6.png  width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Review your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you should see something that looks like a notebook..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/7.png  width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: Try Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have uploaded `titanic.csv`, \n",
    "\n",
    "Try creating a cell, eg., press `a` and entering the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "file_location = \"/FileStore/tables/titanic.csv\"\n",
    "\n",
    "df = (\n",
    "  spark.read.format(\"csv\") \n",
    "  .option(\"inferSchema\", \"true\") \n",
    "  .option(\"header\", \"true\") \n",
    "  .option(\"sep\", \",\") \n",
    "  .load(file_location)\n",
    "  .createOrReplaceTempView(\"titanic\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 11: Associate with Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take awhile on the first run... if it doesn't work, manually associate your notebook with *cluster* you have just created.\n",
    "\n",
    "Top-left dropdown, select your cluster (eg., click on `dummy`)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/8.png width=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 12: Try SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "spark.sql(\"\"\"\n",
    " SELECT AVG(passengers)\n",
    " FROM passengers\n",
    " WHERE ORIGIN = \"JFK\" AND DEST = \"ORD\" AND YEAR = 1990\n",
    "\"\"\").show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I use Spark SQL?\n",
    "\n",
    "If you start a databricks cell with `%sql` then you can omit `spark.sql(''' ''')` and also get syntax highlighting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is spark used in your organization (& any issues)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Problems\n",
    "    * datasets in spark are much larger than your RAM\n",
    "    * pulling in datasets into pandas/python\n",
    "    * WARNING: pandas is for working on a single-machine, \n",
    "* PySpark is sometimes offered *versioned*\n",
    "    * so that certain capabilities are disabled\n",
    "    * & official documentation therf. may not be reflective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (25 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Register (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tasks:\n",
    "    * sign up to databricks community\n",
    "    * import using it's Data facility\n",
    "        * `datasets/titanic.csv` \n",
    "    * read in data in the notebook using the example code above\n",
    "        * set up csv, table name, etc.\n",
    "        * * register as a table view called `titanic`\n",
    "    * run sample sql queries\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (15 min)\n",
    "\n",
    "* Investigate the titanic dataset on spark:\n",
    "\n",
    "* try the following five queries:\n",
    "    * grouping by class:\n",
    "        * class, mean(survived) \n",
    "    * grouping by survival\n",
    "        * mean age, mean fare\n",
    "    * as above\n",
    "        * where adults\n",
    "        * where children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Traditional vs. Spark Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark will only become more performant than traditional solutions at relatively \"extreme\" volumes (/varieties) of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a single csv file into a dataframe on a single machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.82 ms ± 100 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "pd.read_csv('datasets/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a csv file from a *cluster* using a distributed spark query:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/databricks/sparkvspandas.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here spark is c. 1000x slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.68 / (2E-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
